{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Start in KI_Compartment root folder\n",
    "# You must have SymITS2 and all its dependencies installed and working from your home folder (i.e. at ~/SymITS2)\n",
    "# You must have a working version of QIIME, with UCLUST and USEARCH61 installed.\n",
    "# You must have the nucleotide NCBI blast database locally on your machine, and a working version of blast\n",
    "    # You might want to put this on a hard drive, as it is currently ~94GB of data (as of August 1, 2017)\n",
    "    # Download Blastdb from ftp://ftp.ncbi.nlm.nih.gov/blast/documents/blastdb.html, dataset: nt.*tar.gz.\n",
    "    # Then make sure to unzip all folders and place in one folder called \"blastdb\"\n",
    "    # Mine is at \"/media/danielle/CoralDrive/blastdb\"\n",
    "    # For getting blast installed on Linux Ubuntu, this website was helpful: http://www.blopig.com/blog/2014/04/quick-standalone-blast-setup-for-ubuntu-linux/\n",
    "# You must have an updated version of R, I used R 3.4.1 (2017-06-30, Single Candle)\n",
    "    # Required R packages are specified in SymITS2\n",
    "\n",
    "# Prep raw data\n",
    "#chmod u+x data/Bioinf/extract_unzip_prep_rawdata.sh # Make sure script can run\n",
    "#data/Bioinf/extract_unzip_prep_rawdata.sh # This does the following:\n",
    "    # Unzips all original files\n",
    "    # Makes configs for Bokulich qc\n",
    "    # Runs Bokulich qc\n",
    "    # Makes configs for Illumina paired end merging\n",
    "    # Merges Illumina pairs, includes Q30 check\n",
    "    # Filters merged reads\n",
    "    \n",
    "# Or run each command from extract_unzip_prep_rawdata individually...\n",
    "find . -type f -exec gunzip {} + # Unzip these files\n",
    "# They are going to get really big! And this may take quite some time!\n",
    "\n",
    "python data/Bioinf/make_configs.py # Make the Illumina-utils config files for the Bokulich QC method\n",
    "\n",
    "python data/Bioinf/boku_qc.py # Run Bokulich QC method on all applicable files\n",
    "chmod u+x data/Bioinf/boku_qc_KI_Compartment.sh # Give permissions so .sh file will run\n",
    "\n",
    "source ~/virtual-envs/illumina-utils-v2.0.0/bin/activate # Activate Illumina-utils\n",
    "data/Bioinf/boku_qc_KI_Compartment.sh # Run .sh file that was just created \n",
    "\n",
    "python data/Bioinf/make_merge_configs.py # Make the Illumina-utils config files for merging pairs\n",
    "\n",
    "python data/Bioinf/iu_merge_pairs.py # Merge pairs using Illumina-utils\n",
    "chmod u+x data/Bioinf/iu-merge-pairs_KI_Compartment.sh # Give permissions so .sh file will run\n",
    "data/Bioinf/iu-merge-pairs_KI_Compartment.sh # Run .sh file that was just created \n",
    "# find . -maxdepth 1 -name # To check how far along the merging is (because it takes a long time to do hundreds of samples)\n",
    "\n",
    "python data/Bioinf/iu_filter_merged_reads.py # Filter merged reads (MAX-MISMATCH=3) using Illumina-utils\n",
    "chmod u+x data/Bioinf/iu-filter-merged-reads_KI_Compartment.sh # Give permissions so .sh file will run\n",
    "data/Bioinf/iu-filter-merged-reads_KI_Compartment.sh # Run .sh file that was just created \n",
    "\n",
    "python data/Bioinf/rename.py # Rename merged files for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, because I named these samples unneccesarily differently when I sent them in for sequencing, let's fix that before moving downstream...\n",
    "\n",
    "rename 's/DClaar_2014_S_\\.*/KI14SSYM0/' *.fasta\n",
    "rename 's/DClaar_2014_W_\\.*/KI14WSYM0/' *.fasta\n",
    "rename 's/DClaar_2015a_S_\\.*/KI15aSSYM0/' *.fasta\n",
    "rename 's/DClaar_2015a_W_\\.*/KI15aWSYM0/' *.fasta\n",
    "rename 's/DClaar_2015b_W_\\.*/KI15bWSYM0/' *.fasta\n",
    "rename 's/DClaar_2015b_S_\\.*/KI15bSSYM0/' *.fasta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in add_qiime_labels.py: option -i: directory does not exist: 'data/merge/'\n",
      "\n",
      "If you need help with QIIME, see:\n",
      "http://help.qiime.org\n",
      "Error in identify_chimeric_seqs.py: option -i: file does not exist: 'data/fasta/combined_seqs.fna'\n",
      "\n",
      "If you need help with QIIME, see:\n",
      "http://help.qiime.org\n",
      "Error in filter_fasta.py: option -f: file does not exist: 'data/fasta/combined_seqs.fna'\n",
      "\n",
      "If you need help with QIIME, see:\n",
      "http://help.qiime.org\n",
      "cutadapt version 1.14\n",
      "Copyright (C) 2010-2017 Marcel Martin <marcel.martin@scilifelab.se>\n",
      "\n",
      "cutadapt removes adapter sequences from high-throughput sequencing reads.\n",
      "\n",
      "Usage:\n",
      "    cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq\n",
      "\n",
      "For paired-end reads:\n",
      "    cutadapt -a ADAPT1 -A ADAPT2 [options] -o out1.fastq -p out2.fastq in1.fastq in2.fastq\n",
      "\n",
      "Replace \"ADAPTER\" with the actual sequence of your 3' adapter. IUPAC wildcard\n",
      "characters are supported. The reverse complement is *not* automatically\n",
      "searched. All reads from input.fastq will be written to output.fastq with the\n",
      "adapter sequence removed. Adapter matching is error-tolerant. Multiple adapter\n",
      "sequences can be given (use further -a options), but only the best-matching\n",
      "adapter will be removed.\n",
      "\n",
      "Input may also be in FASTA format. Compressed input and output is supported and\n",
      "auto-detected from the file name (.gz, .xz, .bz2). Use the file name '-' for\n",
      "standard input/output. Without the -o option, output is sent to standard output.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Marcel Martin. Cutadapt removes adapter sequences from high-throughput\n",
      "sequencing reads. EMBnet.Journal, 17(1):10-12, May 2011.\n",
      "http://dx.doi.org/10.14806/ej.17.1.200\n",
      "\n",
      "Use \"cutadapt --help\" to see all command-line options.\n",
      "See http://cutadapt.readthedocs.io/ for full documentation.\n",
      "\n",
      "cutadapt: error: [Errno 2] No such file or directory: 'data/fasta/combined_seqs_chimera_filtered.fasta'\n",
      "cutadapt version 1.14\n",
      "Copyright (C) 2010-2017 Marcel Martin <marcel.martin@scilifelab.se>\n",
      "\n",
      "cutadapt removes adapter sequences from high-throughput sequencing reads.\n",
      "\n",
      "Usage:\n",
      "    cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq\n",
      "\n",
      "For paired-end reads:\n",
      "    cutadapt -a ADAPT1 -A ADAPT2 [options] -o out1.fastq -p out2.fastq in1.fastq in2.fastq\n",
      "\n",
      "Replace \"ADAPTER\" with the actual sequence of your 3' adapter. IUPAC wildcard\n",
      "characters are supported. The reverse complement is *not* automatically\n",
      "searched. All reads from input.fastq will be written to output.fastq with the\n",
      "adapter sequence removed. Adapter matching is error-tolerant. Multiple adapter\n",
      "sequences can be given (use further -a options), but only the best-matching\n",
      "adapter will be removed.\n",
      "\n",
      "Input may also be in FASTA format. Compressed input and output is supported and\n",
      "auto-detected from the file name (.gz, .xz, .bz2). Use the file name '-' for\n",
      "standard input/output. Without the -o option, output is sent to standard output.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Marcel Martin. Cutadapt removes adapter sequences from high-throughput\n",
      "sequencing reads. EMBnet.Journal, 17(1):10-12, May 2011.\n",
      "http://dx.doi.org/10.14806/ej.17.1.200\n",
      "\n",
      "Use \"cutadapt --help\" to see all command-line options.\n",
      "See http://cutadapt.readthedocs.io/ for full documentation.\n",
      "\n",
      "cutadapt: error: [Errno 2] No such file or directory: 'data/fasta/trimF.fasta'\n",
      "cutadapt version 1.14\n",
      "Copyright (C) 2010-2017 Marcel Martin <marcel.martin@scilifelab.se>\n",
      "\n",
      "cutadapt removes adapter sequences from high-throughput sequencing reads.\n",
      "\n",
      "Usage:\n",
      "    cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq\n",
      "\n",
      "For paired-end reads:\n",
      "    cutadapt -a ADAPT1 -A ADAPT2 [options] -o out1.fastq -p out2.fastq in1.fastq in2.fastq\n",
      "\n",
      "Replace \"ADAPTER\" with the actual sequence of your 3' adapter. IUPAC wildcard\n",
      "characters are supported. The reverse complement is *not* automatically\n",
      "searched. All reads from input.fastq will be written to output.fastq with the\n",
      "adapter sequence removed. Adapter matching is error-tolerant. Multiple adapter\n",
      "sequences can be given (use further -a options), but only the best-matching\n",
      "adapter will be removed.\n",
      "\n",
      "Input may also be in FASTA format. Compressed input and output is supported and\n",
      "auto-detected from the file name (.gz, .xz, .bz2). Use the file name '-' for\n",
      "standard input/output. Without the -o option, output is sent to standard output.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Marcel Martin. Cutadapt removes adapter sequences from high-throughput\n",
      "sequencing reads. EMBnet.Journal, 17(1):10-12, May 2011.\n",
      "http://dx.doi.org/10.14806/ej.17.1.200\n",
      "\n",
      "Use \"cutadapt --help\" to see all command-line options.\n",
      "See http://cutadapt.readthedocs.io/ for full documentation.\n",
      "\n",
      "cutadapt: error: [Errno 2] No such file or directory: 'data/fasta/trimF2.fasta'\n",
      "cutadapt version 1.14\n",
      "Copyright (C) 2010-2017 Marcel Martin <marcel.martin@scilifelab.se>\n",
      "\n",
      "cutadapt removes adapter sequences from high-throughput sequencing reads.\n",
      "\n",
      "Usage:\n",
      "    cutadapt -a ADAPTER [options] [-o output.fastq] input.fastq\n",
      "\n",
      "For paired-end reads:\n",
      "    cutadapt -a ADAPT1 -A ADAPT2 [options] -o out1.fastq -p out2.fastq in1.fastq in2.fastq\n",
      "\n",
      "Replace \"ADAPTER\" with the actual sequence of your 3' adapter. IUPAC wildcard\n",
      "characters are supported. The reverse complement is *not* automatically\n",
      "searched. All reads from input.fastq will be written to output.fastq with the\n",
      "adapter sequence removed. Adapter matching is error-tolerant. Multiple adapter\n",
      "sequences can be given (use further -a options), but only the best-matching\n",
      "adapter will be removed.\n",
      "\n",
      "Input may also be in FASTA format. Compressed input and output is supported and\n",
      "auto-detected from the file name (.gz, .xz, .bz2). Use the file name '-' for\n",
      "standard input/output. Without the -o option, output is sent to standard output.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Marcel Martin. Cutadapt removes adapter sequences from high-throughput\n",
      "sequencing reads. EMBnet.Journal, 17(1):10-12, May 2011.\n",
      "http://dx.doi.org/10.14806/ej.17.1.200\n",
      "\n",
      "Use \"cutadapt --help\" to see all command-line options.\n",
      "See http://cutadapt.readthedocs.io/ for full documentation.\n",
      "\n",
      "cutadapt: error: [Errno 2] No such file or directory: 'data/fasta/trimF2_trimR.fasta'\n",
      "rm: cannot remove 'data/fasta/trimF.fasta': No such file or directory\n",
      "rm: cannot remove 'data/fasta/trimF2.fasta': No such file or directory\n",
      "rm: cannot remove 'data/fasta/trimF2_trimR.fasta': No such file or directory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Next, start using SymITS2 by Ross Cunning https://github.com/jrcunning/SymITS2\n",
    "~/SymITS2/qc_trim_reads.sh # This does the following:\n",
    "    # Adds QIIME labels\n",
    "    # Identifies chimeric sequences\n",
    "    # Filters out chimeric sequences\n",
    "    # Runs cutadapt 4 times\n",
    "    # Removes intermediate files from cutadapt process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7332473  : data/fasta/combined_seqs_trimmed.fasta (Sequence lengths (mean +/- std): 306.5629 +/- 24.1155)\n",
      "7332473  : Total\n"
     ]
    }
   ],
   "source": [
    "count_seqs.py -i data/fasta/combined_seqs_trimmed.fasta # Count how many seqs there are after quality filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using programs that use GNU Parallel to process data for publication please cite:\n",
      "\n",
      "  O. Tange (2011): GNU Parallel - The Command-Line Power Tool,\n",
      "  ;login: The USENIX Magazine, February 2011:42-47.\n",
      "\n",
      "This helps funding further development; and it won't cost you a cent.\n",
      "Or you can get GNU Parallel without this requirement by paying 10000 EUR.\n",
      "\n",
      "To silence this citation notice run 'parallel --bibtex' once or use '--no-notice'.\n",
      "\n",
      "When using programs that use GNU Parallel to process data for publication please cite:\n",
      "\n",
      "  O. Tange (2011): GNU Parallel - The Command-Line Power Tool,\n",
      "  ;login: The USENIX Magazine, February 2011:42-47.\n",
      "\n",
      "This helps funding further development; and it won't cost you a cent.\n",
      "Or you can get GNU Parallel without this requirement by paying 10000 EUR.\n",
      "\n",
      "To silence this citation notice run 'parallel --bibtex' once or use '--no-notice'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From SymITS2 by Ross Cunning https://github.com/jrcunning/SymITS2\n",
    "# Cluster at 97% within samples (each sample is clustered independently)\n",
    "~/SymITS2/otus_97_bysample.sh data/fasta/combined_seqs_trimmed.fasta data/Bioinf/clust\n",
    "# Arguments are 1) Combined, trimmed sequences (fasta file) and 2)\n",
    "Output directory\n",
    "# Creates data/Bioinf/clust/all_rep_set_rep_set.fasta for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R version 3.4.1 (2017-06-30) -- \"Single Candle\"\n",
      "Copyright (C) 2017 The R Foundation for Statistical Computing\n",
      "Platform: x86_64-pc-linux-gnu (64-bit)\n",
      "\n",
      "R is free software and comes with ABSOLUTELY NO WARRANTY.\n",
      "You are welcome to redistribute it under certain conditions.\n",
      "Type 'license()' or 'licence()' for distribution details.\n",
      "\n",
      "  Natural language support but running in an English locale\n",
      "\n",
      "R is a collaborative project with many contributors.\n",
      "Type 'contributors()' for more information and\n",
      "'citation()' on how to cite R or R packages in publications.\n",
      "\n",
      "Type 'demo()' for some demos, 'help()' for on-line help, or\n",
      "'help.start()' for an HTML browser interface to help.\n",
      "Type 'q()' to quit R.\n",
      "\n",
      "> # Get command line arguments\n",
      "> args = commandArgs(trailingOnly=TRUE)\n",
      "> # If two arguments not provided, return an error\n",
      "> if (length(args) < 2) {\n",
      "+   stop(\"must specify query sequences (.fasta) and reference sequences (.fasta)\", call.=FALSE)\n",
      "+ }\n",
      "> \n",
      "> # Import query sequences\n",
      "> library(Biostrings)\n",
      "Loading required package: BiocGenerics\n",
      "Loading required package: parallel\n",
      "\n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "The following objects are masked from ‘package:parallel’:\n",
      "\n",
      "    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n",
      "    clusterExport, clusterMap, parApply, parCapply, parLapply,\n",
      "    parLapplyLB, parRapply, parSapply, parSapplyLB\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, append, as.data.frame, cbind, colMeans, colnames,\n",
      "    colSums, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n",
      "    grepl, intersect, is.unsorted, lapply, lengths, Map, mapply, match,\n",
      "    mget, order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n",
      "    rbind, Reduce, rowMeans, rownames, rowSums, sapply, setdiff, sort,\n",
      "    table, tapply, union, unique, unsplit, which, which.max, which.min\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "Loading required package: stats4\n",
      "\n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    expand.grid\n",
      "\n",
      "Loading required package: IRanges\n",
      "Loading required package: XVector\n",
      "\n",
      "Attaching package: ‘Biostrings’\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    strsplit\n",
      "\n",
      "> otus <- readDNAStringSet(args[1])\n",
      "Error in .Call2(\"new_input_filexp\", filepath, PACKAGE = \"XVector\") : \n",
      "  cannot open file 'data/Bioinf/clust/all_rep_set_rep_set.fasta'\n",
      "Calls: readDNAStringSet ... lapply -> FUN -> new_input_filexp -> .Call2 -> .Call\n",
      "Execution halted\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# From SymITS2 by Ross Cunning https://github.com/jrcunning/SymITS2\n",
    "# Use global alignment to identify sequences\n",
    "R --vanilla < ~/SymITS2/run_nw.R --args data/Bioinf/clust/all_rep_set_rep_set.fasta data/ITS2db_trimmed_derep.fasta\n",
    "# Arguments are 1) query sequences - rep set fasta file, and 2) reference database sequences in .fasta format\n",
    "# Creates data/Bioinf/clust/all_rep_set_rep_set_nw_tophits.tsv for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now extract metadata from full metadata spreadsheet\n",
    "# Using mapping_file.txt\n",
    "R --vanilla < data/Coralphoto__Metadata/process_coral_metadata.R \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From SymITS2 by Ross Cunning https://github.com/jrcunning/SymITS2\n",
    "# Build a phyloseq object\n",
    "R --vanilla < ~/SymITS2/build_phyloseq.R --args data/Bioinf/clust/all_rep_set_rep_set_nw_tophits.tsv data/Coralphoto__Metadata/KI_Compartment_metadata.tsv data/Bioinf/clust/97_otus_bysample.tsv data/ITS2db_trimmed_notuniques_otus.txt analyses/KI_Compartment.RData\n",
    "# Arguments are 1) nw_tophits - taxonomic assignment output from run_nw.R, 2) sample metadata in tsv format, 3) OTU table, 4) duplicate reference taxa names, 5) output filename\n",
    "# Creates analyses/KI_Compartment.RData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From SymITS2 by Ross Cunning https://github.com/jrcunning/SymITS2\n",
    "# Filter out sequences which do not blast to Symbiodnium\n",
    "R --vanilla < ~/SymITS2/filter_notsym.R --args analyses/KI_Compartment.RData data/Bioinf/clust/all_rep_set_rep_set.fasta analyses/KI_Compartment.RData /media/danielle/CoralDrive/blastdb\n",
    "# Output file is KI_Compartment.RData for downstream analysis (next step = analyses/KI_Compartment_filter_samples.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the phylogenetic tree to add to phyloseq object\n",
    "\n",
    "#Originally from Moorea_Sym pipeline\n",
    "#data/Bioinf/clust/all_rep_set_rep_set.fasta = seqs\n",
    "#data/tax_table.txt = ISDs\n",
    "#column 9 is clade ID\n",
    "#awk '{print $1}' data/tax_table.txt = denovo name\n",
    "#awk '{print $8}' data/tax_table.txt = clade name\n",
    "\n",
    "#subset ID lists by clade\n",
    "awk '$9 ~ /^A/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/A_tree_seq.ids\n",
    "awk '$9 ~ /^B/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/B_tree_seq.ids\n",
    "awk '$9 ~ /^C/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/C_tree_seq.ids\n",
    "awk '$9 ~ /^D/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/D_tree_seq.ids\n",
    "awk '$9 ~ /^E/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/E_tree_seq.ids\n",
    "awk '$9 ~ /^F/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/F_tree_seq.ids\n",
    "awk '$9 ~ /^G/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/G_tree_seq.ids\n",
    "awk '$9 ~ /^H/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/H_tree_seq.ids\n",
    "awk '$9 ~ /^I/{ print $1; }' data/tax_table.txt > data/Bioinf/tree/I_tree_seq.ids\n",
    "\n",
    "#filter seqs by id list\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/A_tree_seq.ids -o data/Bioinf/tree/A_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/B_tree_seq.ids -o data/Bioinf/tree/B_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/C_tree_seq.ids -o data/Bioinf/tree/C_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/D_tree_seq.ids -o data/Bioinf/tree/D_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/E_tree_seq.ids -o data/Bioinf/tree/E_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/F_tree_seq.ids -o data/Bioinf/tree/F_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/G_tree_seq.ids -o data/Bioinf/tree/G_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/H_tree_seq.ids -o data/Bioinf/tree/H_tree_seqs.fasta\n",
    "filter_fasta.py -f data/Bioinf/clust/all_rep_set_rep_set.fasta -s data/Bioinf/tree/I_tree_seq.ids -o data/Bioinf/tree/I_tree_seqs.fasta\n",
    "\n",
    "#check seq number\n",
    "grep -c \">\" data/Bioinf/tree/A_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/B_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/C_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/D_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/E_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/F_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/G_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/H_tree_seqs.fasta\n",
    "grep -c \">\" data/Bioinf/tree/I_tree_seqs.fasta\n",
    "\n",
    "#align fasta files for each clade\n",
    "align_seqs.py -i data/Bioinf/tree/A_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/B_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/C_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/D_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/E_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/F_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/G_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/H_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "align_seqs.py -i data/Bioinf/tree/I_tree_seqs.fasta -m muscle -o data/Bioinf/tree/\n",
    "\n",
    "#remove extra header info from alignments\n",
    "sed 's/ .*//' data/Bioinf/tree/A_tree_seqs_aligned.fasta > data/Bioinf/tree/A_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/B_tree_seqs_aligned.fasta > data/Bioinf/tree/B_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/C_tree_seqs_aligned.fasta > data/Bioinf/tree/C_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/D_tree_seqs_aligned.fasta > data/Bioinf/tree/D_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/E_tree_seqs_aligned.fasta > data/Bioinf/tree/E_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/F_tree_seqs_aligned.fasta > data/Bioinf/tree/F_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/G_tree_seqs_aligned.fasta > data/Bioinf/tree/G_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/H_tree_seqs_aligned.fasta > data/Bioinf/tree/H_tree_seqs_aligned_clean.fasta\n",
    "sed 's/ .*//' data/Bioinf/tree/I_tree_seqs_aligned.fasta > data/Bioinf/tree/I_tree_seqs_aligned_clean.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format and filter the RData file\n",
    "R --vanilla < analyses/KI_Compartment_filter_samples.R\n",
    "# Input file is analyses/KI_Compartment.RData\n",
    "# Must run build_phy_tree first for this to work properly\n",
    "# Output file is: data/KI_seqs_f_coral_grouped.RData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rarefaction\n",
    "# Multiple rarefactions to look at different sequence rarefaction depths\n",
    "# 1000 to 10000 in steps of 1000\n",
    "multiple_rarefactions.py -i data/Bioinf/clust/97_otus_bysample.biom -o data/Bioinf/clust/rarefy/mult -m 1000 -x 10000 -s 1000\n",
    "# 200 to 1000 in steps of 100\n",
    "multiple_rarefactions.py -i data/Bioinf/clust/97_otus_bysample.biom -o data/Bioinf/clust/rarefy/mult -m 200 -x 1000 -s 100\n",
    "\n",
    "# Single rarefaction to 1000 seqs\n",
    "single_rarefaction.py -i data/Bioinf/clust/97_otus_bysample.biom -o data/Bioinf/clust/rarefy/rarefaction_1000.biom --depth 1000\n",
    "# Convert .biom to tsv\n",
    "biom convert -i data/Bioinf/clust/rarefy/rarefaction_1000.biom -o data/Bioinf/clust/rarefy/rarefaction_1000.tsv --to-tsv\n",
    "# Build a phyloseq object\n",
    "R --vanilla < ~/SymITS2/build_phyloseq.R --args data/Bioinf/clust/all_rep_set_rep_set_nw_tophits.tsv data/Coralphoto__Metadata/KI_Compartment_metadata.tsv data/Bioinf/clust/rarefy/rarefaction_1000.tsv data/ITS2db_trimmed_notuniques_otus.txt analyses/KI_Compartment_rarefy.RData\n",
    "# Arguments are 1) nw_tophits - taxonomic assignment output from run_nw.R, 2) sample metadata in tsv format, 3) OTU table, 4) duplicate reference taxa names, 5) output filename\n",
    "# Creates analyses/KI_Compartment.RData\n",
    "\n",
    "# Filter out sequences which do not blast to Symbiodnium\n",
    "R --vanilla < ~/SymITS2/filter_notsym.R --args analyses/KI_Compartment_rarefy.RData data/Bioinf/clust/all_rep_set_rep_set.fasta analyses/KI_Compartment_rarefy.RData /media/danielle/CoralDrive/blastdb\n",
    "\n",
    "# Build the phylogenetic tree to add to phyloseq object\n",
    "chmod u+x data/Bioinf/tree/build_phy_tree_rare.sh # Make sure script can run\n",
    "data/Bioinf/tree/build_phy_tree_rare.sh # Run build_phy_tree script\n",
    "\n",
    "# Format and filter the RData file\n",
    "R --vanilla < analyses/KI_Compartment_filter_samples_rare.R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
